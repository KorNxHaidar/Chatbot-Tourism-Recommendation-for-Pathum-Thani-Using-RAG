from openai import OpenAI
import streamlit as st
import os
import dotenv
import uuid
import time
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from rag_variables import (
    vectorstore, 
    retriever, 
    typhoon_prompt, 
    embeddings,
)

# Load environment variables (API key)
dotenv.load_dotenv()

typhoon_token = os.getenv("TYPHOON_API_KEY")
# Set up Typhoon API
client = OpenAI(
    api_key=typhoon_token,
    base_url="https://api.opentyphoon.ai/v1"  # URL of Typhoon API
)

# Set up Streamlit
st.set_page_config(
    page_title="Typhoon LLM", 
    page_icon="üìö", 
    layout="centered", 
    initial_sidebar_state="expanded"
)

# --- Initial Setup ---
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []

# ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
def type_effect(message, speed=0.05):
    """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß"""
    full_message = ""
    message_placeholder = st.empty()  # Placeholder ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    for i in range(len(message)):
        full_message += message[i]
        message_placeholder.markdown(f"<h1 style='text-align: center; font-size: 4.5em; font-weight: bold;'>{full_message}</h1>", unsafe_allow_html=True)
        time.sleep(speed)  # ‡∏£‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß

    # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏£‡∏≠‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
    time.sleep(1.5)  # ‡∏£‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß
    for i in range(len(message), -1, -1):
        message_placeholder.markdown(f"<h1 style='text-align: center; font-size: 4em; font-weight: bold;'>{message[:i]}</h1>", unsafe_allow_html=True)
        time.sleep(speed)  # ‡∏£‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥ type_effect ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
if 'type_effect_done' not in st.session_state:
    st.session_state.type_effect_done = False

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥ type_effect ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô session_state.messages
if not st.session_state.messages and not st.session_state.type_effect_done:
    type_effect("What can I help with?", speed=0.05)
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤ type_effect ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
    st.session_state.type_effect_done = True

# Function to generate a response using the Typhoon LLM with type effect
def generate_response(context, chat_history, question, message_placeholder):
    # Format the chat history correctly
    history = "\n".join([f"{entry['role'].capitalize()}: {entry['content']}" for entry in chat_history])
    prompt = typhoon_prompt.format(context=context, question=question) 
    chat_completion = client.chat.completions.create(
        model="typhoon-v1.5x-70b-instruct",
        messages=[{"role": "user", "content": prompt}],
    )

    full_response = chat_completion.choices[0].message.content
    
    # Display the response with type effect
    for i in range(len(full_response)):
        message_placeholder.markdown(full_response[:i+1])  # Show text progressively
        time.sleep(0.02)  # Adjust the speed of the typing effect

    return full_response

# Function to answer the user's question
def answer_question(user_question, chat_history):
    retrieved_contexts = retriever.get_relevant_documents(user_question)
    context = "\n".join([doc.page_content for doc in retrieved_contexts])
    response = generate_response(context=context, chat_history=chat_history, question=user_question)
    return response 

# --- Main Chat App ---
# Display welcome message with type effect
if "type_effect_done" not in st.session_state:
    st.session_state.type_effect_done = False

def type_effect(message, speed=0.05):
    """Display message with typing effect"""
    full_message = ""
    message_placeholder = st.empty()  # Placeholder to display the message
    for i in range(len(message)):
        full_message += message[i]
        message_placeholder.markdown(f"<h1 style='text-align: center; font-size: 4.5em; font-weight: bold;'>{full_message}</h1>", unsafe_allow_html=True)
        time.sleep(speed)

    # Wait after the message is fully displayed
    time.sleep(1.5)
    for i in range(len(message), -1, -1):
        message_placeholder.markdown(f"<h1 style='text-align: center; font-size: 4em; font-weight: bold;'>{message[:i]}</h1>", unsafe_allow_html=True)
        time.sleep(speed)

# Initial message on first load
if not st.session_state.messages and not st.session_state.type_effect_done:
    type_effect("What can I help with?", speed=0.05)
    st.session_state.type_effect_done = True

# Handle incoming user input and chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Main chat flow where the assistant's response will be shown with type effect
if prompt := st.chat_input("Message Typhoon Chat"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        messages = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
        full_response = generate_response(prompt, messages, prompt, message_placeholder)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
